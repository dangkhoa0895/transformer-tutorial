{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dangkhoa0895/transformer-tutorial/blob/master/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2ENKuobJQXp"
      },
      "source": [
        "# Giới Thiệu Transformer\n",
        "Mô hình Transformer ra đời là một bước tiến dài của Google, vì nó chính là nền tảng của rất nhiều mô hình khác mà nổi tiếng nhất là BERT (Bidirectional Encoder Representations from Transformers) một mô hình dùng để học biểu diễn của từ tốt nhất hiện tại và đã tạo ra một bước ngoặc lớn cho động đồng NLP từ năm 2019. Và chính Google cũng đã áp dụng BERT trong cỗ máy tìm kiếm của họ. Để hiểu BERT, ta cần phải nắm rõ về mô hình Transformer.\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/dangkhoa0895/transformer-tutorial/master/res/bert.jpg)\n",
        "\n",
        "Ý tưởng chủ đạo của Transformer vẫn là áp dụng cơ chế Attention, những ở mức phức tạp hơn và thật sự là thú vị hơn so với cách được đề xuất trước đó."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb7MbGxAJnBW"
      },
      "source": [
        "## Tổng Quan Mô Hình\n",
        "Để cho dễ cảm nhận được cách mà mô hình hoạt động, ta sẽ đi qua toàn bộ kiến trúc của mô hình ở mức high-level và sau đó sẽ đi chi tiết từng phần nhỏ cũng như công thức toán của nó.\n",
        "\n",
        "Giống như những mô hình dịch máy khác, kiến trúc tổng quan của mô hình transformer bao gồm 2 phần lớn là encoder và decoder. Encoder dùng để học vector biểu của câu với mong muốn rằng vector này mang thông tin hoàn hảo của câu đó. Decoder thực hiện chức năng chuyển vector biểu diễn kia thành ngôn ngữ đích.\n",
        "\n",
        "Trong ví dụ ở dưới, encoder của mô hình transformer nhận một câu tiếng Việt, và encode thành một vector biểu diễn ngữ nghĩa của câu little sun, sau đó mô hình decoder nhận vector biểu diễn này, và dịch nó thành câu tiếng Việt mặt trời bé nhỏ\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/dangkhoa0895/transformer-tutorial/master/res/overview.jpg)\n",
        "\n",
        "Một trong những ưu điểm của transformer là mô hình này có khả năng xử lý song song cho các từ. Như ta thấy, Encoders của mô hình transfomer là một dạng feedforward neural networks, bao gồm nhiều encoder layer khác, mỗi encoder layer này xử lý đồng thời các từ. Trong khi đó, với mô hình LSTM, thì các từ phải được xử lý tuần tự. Ngoài ra, mô hình Transformer còn xử lý câu đầu vào theo 2 hướng mà không cần phải stack thêm một môt hình LSTM nữa như trong kiến trúc Bidirectional LSTM.\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/dangkhoa0895/transformer-tutorial/master/res/overview2.jpg)\n",
        "\n",
        "Một cái nhìn vừa tổng quát và chi tiết sẽ giúp ích cho chúng ta. Tiếp theo ta sẽ đi vào chi tiết một số phần cực kì quan trọng như position encoding, multi head attention của encoder, còn của decoder thì kiến trúc rất giống với của encoder, do đó ta sẽ chỉ đi nhanh qua mà thôi.\n",
        "\n",
        "![](https://raw.githubusercontent.com/dangkhoa0895/transformer-tutorial/master/res/overview3.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM1dnT0JKkdk"
      },
      "source": [
        "# Embedding Layer with Position Encoding\n",
        "Trước khi đi vào mô hình encoder, ta sẽ tìm hiểu cơ chế rất thú vị là Position Encoding dùng để đưa thông tin về vị trí của các từ vào mô hình transformer.\n",
        "\n",
        "Đầu tiên, các từ được biểu diễn bằng một vector sử dụng một ma trận word embedding có số dòng bằng kích thước của tập từ vựng. Sau đó các từ trong câu được tìm kiếm trong ma trận này, và được nối nhau thành các dòng của một ma trận 2 chiều chứa ngữ nghĩa của từng từ riêng biệt. Nhưng như ta đã thấy, transformer xử lý các từ song song, do đó, với chỉ word embedding mô hình không thể nào biết được vị trí các từ. Như vậy, chúng ta cần một cơ chế nào đó để đưa thông tin vị trí các từ vào trong vector đầu vào. Đó là lúc positional encoding xuất hiện và giải quyết vấn đề của chúng ta. Tuy nhiên, trước khi giới thiệu cơ chế position encoding của tác giả, ta có thể giải quyết vấn đề băng một số cách naive như sau:\n",
        "\n",
        "Biểu diễn vị trí các từ bằng chuỗi các số liên tục từ 0,1,2,3 …, n. Tuy nhiên, chúng ta gặp ngay vấn đề là khi chuỗi dài thì số này có thể khá lớn, và mô hình sẽ gặp khó khăn khi dự đoán những câu có chiều dài lớn hơn tất cả các câu có trong tập huấn luyện. Để giải quyết vấn đề này, ta có thể chuẩn hóa lại cho chuỗi số này nằm trong đoạn từ 0-1 bằng cách chia cho n nhưng mà chúng ta sẽ gặp vấn đề khác là khoảng cách giữ 2 từ liên tiếp sẽ phụ thuộc vào chiều dài của chuỗi, và trong một khoản cố định, chúng ta không hình dung được khoản đó chứa bao nhiêu từ. Điều này có nghĩa là ý nghĩa của position encoding sẽ khác nhau tùy thuộc vào độ dài của câu đó."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGlFpeIQKss-"
      },
      "source": [
        "# Phương pháp đề xuất sinusoidal position encoding\n",
        "Phương pháp của tác giả đề xuất không gặp những hạn chế mà chúng ta vừa nêu. Vị trí của các từ được mã hóa bằng một vector có kích thước bằng word embedding và được cộng trực tiếp vào word embedding.\n",
        "\n",
        "![](https://raw.githubusercontent.com/dangkhoa0895/transformer-tutorial/master/res/embedding.jpg)\n",
        "\n",
        "Cụ thể, tại vị trí chẵn, tác giả sử dụng hàm sin, và với vị trí lẻ tác giả sử dụng hàm cos để tính giá trị tại chiều đó.\n",
        "\n",
        "![alt text](https://github.com/pbcquoc/pbcquoc.github.io/raw/master/images/transformer/pe_formula.png)\n",
        "\n",
        "Trong hình dưới này, minh họa cho cách tính position encoding của tác giả. Giả sử chúng ta có word embedding có 6 chiều, thì position encoding cũng có tương ứng là 6 chiều. Mỗi dòng tương ứng với một từ. Giá trị của các vector tại mỗi vị trí được tính toán theo công thức ở hình dưới.\n",
        "\n",
        "![](https://pbcquoc.github.io/images/transformer/pe.png)\n",
        "\n",
        "Lúc này một số bạn sẽ thắc mắc tại sao với cách biểu diện vị trí như tác giả đề xuất lại có thể mã hóa thông tin vị trí của từ? Hãy tưởng tượng bạn có các số từ 0-15. Các bạn có thể thấy rằng bit ngoài cùng bên phải thay đổi nhanh nhất mỗi 1 số, và sau đó là bit bên phải thứ 2, thay đổi mỗi 2 số, tương tự cho các bit khác.\n",
        "\n",
        "![](https://pbcquoc.github.io/images/transformer/pe_intuition.jpg)\n",
        "\n",
        "Trong công thức của tác giả đề xuất, ta cũng thấy rằng, hàm sin và cos có dạng đồ thị tần số và tần số này giảm dần ở các chiều lớn dần. Ta xem hình dưới, ở chiều 0, giá trị thay đổi liên tục tương ứng với màu sắc thay đổi liên tục, và tần số thay đổi này giảm dần ở các chiều lớn hơn.\n",
        "\n",
        "![](https://pbcquoc.github.io/images/transformer/pe_heatmap.png)\n",
        "\n",
        "Nên chúng ta có thể cảm nhận được việc biểu diễn của tác giả khá tương tự như cách biểu diễn các số nguyên trong hệ nhị phân, cho nên chúng ta có thể biểu diễn được vị trí các từ theo cách như vậy.\n",
        "\n",
        "Chúng ta cũng có thể xem ma trận khoảng cách của các vector biểu diễn vị trí như hình dưới. Rõ ràng, các vector biểu diễn thể hiện được tính chất khoảng cách giữa 2 từ. 2 từ cách càng xa nhau thì khoảng cách càng lớn hơn.\n",
        "\n",
        "![](https://pbcquoc.github.io/images/transformer/pe_distance.png)\n",
        "\n",
        "Ngoài ra, một tính chất của phương pháp tác giả đề xuất là nó cho phép mô hình dễ dàng học được mối quan hệ tương đối giữ các từ. Cụ thể, biểu diễn vị trí của từ t + offset có thể chuyển thành biểu diễn vị trí của từ t bằng một phép biến đổi tuyến tính dựa trên ma trận phép quay.\n",
        "\n",
        "Để dễ hình dung phương pháp của tác giả đề xuất lại hoạt động tốt, ta có thể tưởng tượng, hàm sin, và cos, giống như là kim giây và kim phút trên đồng hồ. Với 2 kim này, chúng ta có thể biểu diễn được 3600 vị trí. Và đồng thời có thể hiểu được ngay tại sao biểu diễn của từ t + offset và từ t lại có thể dễ dàng chuyển đổi cho nhau.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdEQKhVvNiSz"
      },
      "source": [
        "# Encoder\n",
        "Encoder của mô hình transformer có thể bao gồm nhiều encoder layer tượng tự nhau. Mỗi encoder layer của transformer lại bao gồm 2 thành phần chính là multi head attention và feedforward network, ngoài ra còn có cả skip connection và normalization layer.\n",
        "\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/dangkhoa0895/transformer-tutorial/master/res/encoder.jpg)\n",
        "\n",
        "Encoder đầu tiên sẽ nhận ma trận biểu diễn của các từ đã được cộng với thông tin vị trí thông qua positional encoding. Sau đó, ma trận này sẽ được xử lý bởi Multi Head Attention. Multi Head Attention thật chất là self-attention, nhưng mà để mô hình có thể có chú ý nhiều pattern khác nhau, tác giả đơn giản là sử dụng nhiều self-attention.\n",
        "\n",
        "## Self Attention Layer\n",
        "Self Attention cho phép mô hình khi mã hóa một từ có thể sử dụng thông tin của những từ liên quan tới nó. Ví dụ khi từ nó được mã hóa, nó sẽ chú ý vào các từ liên quan như là mặt trời. Cơ chế self attention này có ý nghĩa tương tự như cơ chế attention.\n",
        "\n",
        "![alt text](https://pbcquoc.github.io/images/transformer/self_attention.jpg)\n",
        "\n",
        "Ta có thể tưởng tượng cơ chế self attention giống như cơ chế tìm kiếm. Với một từ cho trước, cơ chế này sẽ cho phép mô hình tìm kiếm trong cách từ còn lại, từ nào “giống” để sau đó thông tin sẽ được mã hóa dựa trên tất cả các từ trên.\n",
        "\n",
        "Đầu tiên, với môi từ chúng ta cần tạo ra 3 vector: query, key, value vector bằng cách nhân ma trận biểu diễn các từ đầu vào với ma trận học tương ứng.\n",
        "\n",
        "* query vector: vector dùng để chứa thông tin của từ được tìm kiếm, so sách. Giống như là câu query của google search.\n",
        "* key vector: vector dùng để biểu diễn thông tin các từ được so sánh với từ cần tìm kiếm ở trên. Ví dụ, như các trang webs mà google sẽ so sánh với từ khóa mà bạn tìm kiếm.\n",
        "* value vector: vector biểu diễn nội dung, ý nghĩa của các từ. Ta có thể tượng tượng, nó như là nội dung trang web được hiển thị cho người dùng sau khi tìm kiếm.\n",
        "Để tính tương quan, chúng ta đơn giản chỉ cần tính tích vô hướng dựa các vector query và key. Sau đó dùng hàm softmax để chuẩn hóa chỉ số tương quan trong đoạn 0-1, và cuối cùng, tính trung bình cộng có trọng số giữa các vector values sử dụng chỉ số tương quan mới tính được. Quá dễ !!!\n",
        "\n",
        "![](https://pbcquoc.github.io/images/transformer/self_attention_2.png)\n",
        "\n",
        "Cụ thể hơn, quá trình tính toán attention vector có thể được tóm tắt làm 3 bước như sau:\n",
        "\n",
        "* Bước 1: Tính ma trận query, key, value bằng cách khởi tạo 3 ma trận trọng số query, key, vector. Sau đó nhân input với các ma trận trọng số này để tạo thành 3 ma trận tương ứng.\n",
        "* Bước 2: Tính attention weights. Nhân 2 ma trận key, query vừa được tính ở trên với nhau để với ý nghĩa là so sánh giữ câu query và key để học mối tương quan. Sau đó thì chuẩn hóa về đoạn [0-1] bằng hàm softmax. 1 có nghĩa là câu query giống với key, 0 có nghĩa là không giống.\n",
        "* Bước 3: Tính output. Nhân attention weights với ma trận value. Điều này có nghĩa là chúng ta biểu diễn một từ bằng trung bình có trọng số (attention weights) của ma trận value.\n",
        "\n",
        "![](https://pbcquoc.github.io/images/transformer/attention_vector.jpg)\n",
        "\n",
        "## Multi Head Attention\n",
        "Chúng ta muốn mô hình có thể học nhiều kiểu mối quan hệ giữ các từ với nhau. Với mỗi self-attention, chúng ta học được một kiểu pattern, do đó để có thể mở rộng khả năng này, chúng ta đơn giản là thêm nhiều self-attention. Tức là chúng ta cần nhiều ma trận query, key, value mà thôi. Giờ đây ma trận trọng số key, query, value sẽ có thêm 1 chiều depth nữa.\n",
        "\n",
        "![](https://pbcquoc.github.io/images/transformer/multi_head_attention.jpg)\n",
        "\n",
        "Multi head attention cho phép mô hình chú ý đến đồng thời những pattern dễ quan sát được như sau.\n",
        "\n",
        "* Chú ý đến từ kế trước của một từ\n",
        "* Chú ý đến từ kế sau của một từ\n",
        "* Chú ý đến những từ liên quan của một từ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvOrq4-WPXYK"
      },
      "source": [
        "# Residuals Connection và Normalization Layer\n",
        "\n",
        "Trong kiến trúc của mô hình transformer, residuals connection và mormalization layer được sử dụng mọi nơi, giống như tinh thần của nó. 2 kỹ thuật giúp cho mô hình huấn luyện nhanh hội tụ hơn và trách mất mát thông tin trong quá trình huấn luyện mô hình, ví dụ như là thông tin của vị trí các từ được mã hóa.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48SiXL_zQQ9C"
      },
      "source": [
        "# Decoder\n",
        "Decoder thực hiện chức năng giải mã vector của câu nguồn thành câu đích, do đó decoder sẽ nhận thông tin từ encoder là 2 vector key và value. Kiến trúc của decoder rất giống với encoder, ngoại trừ có thêm một multi head attention nằm ở giữa dùng để học mối liên quan giữa từ đang được dịch với các từ được ở câu nguồn.\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/dangkhoa0895/transformer-tutorial/master/res/decoder.jpg)\n",
        "\n",
        "## Masked Multi Head Attention\n",
        "Masked Multi Head Attention tất nhiên là multi head attention mà chúng ta đã nói đến ở trên, có chức năng dùng để encode các từ câu đích trong quá trình dịch, tuy nhiên, lúc cài đặt chúng ta cần lưu ý rằng phải che đi các từ ở tương lai chưa được mô hình dịch đến, để làm việc này thì đơn giản là chúng ta chỉ cần nhân với một vector chứa các giá trị 0,1.\n",
        "\n",
        "Trong decoder còn có một multi head attention khác có chức năng chú ý các từ ở mô hình encoder, layer này nhận vector key và value từ mô hình encoder, và output từ layer phía dưới. Đơn giản bởi vì chúng ta muốn so sánh sự tương quan giữ từ đang được dịch vời các từ nguồn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKJoYats5LYn"
      },
      "source": [
        "# Label Smoothing\n",
        "Với mô hình nhiều triệu tham số của transformer, thì việc overfit là chuyện dễ dàng xảy ra. Để hạn chế hiện tượng overfit, ta có thể sử dụng kỹ thuật label smoothing. Về cơ bản thì ý tưởng của kỹ thuật này khá đơn giản, chúng ta sẽ phạt mô hình khi nó quá tự tin vào việc dự đoán của mình. Thay vì mã hóa nhãn là một one-hot vector, ta sẽ thay đổi nhãn này một chút bằng cách phân bố một tí xác suất vào các trường hợp còn lại.\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/dangkhoa0895/transformer-tutorial/master/res/label_smoothing.jpg)\n",
        "\n",
        "Giờ thì ta sẽ an tâm khi có thể để số epoch lớn mà không lo rằng mô hình sẽ overfit nặng nề.\n",
        "\n"
      ]
    }
  ]
}